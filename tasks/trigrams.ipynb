{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Large Language Model Processing**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Third-order letter approximation model\n",
    "\n",
    "In this task, we build a trigram-based model of the English language by processing texts from Project Gutenberg. The steps include sanitizing the text, removing unwanted characters, and counting the frequency of trigrams (sequences of three characters) in the text.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Sanitization\n",
    "\n",
    "We remove any preamble and postamble specific to Project Gutenberg texts and restrict the character set to uppercase ASCII letters, spaces, and full stops. All other characters are removed.\n",
    "\n",
    "The `sanitize_and_trim()` function is responsible for this task. It cleans the text as follows:\n",
    "1. Converts all letters to uppercase.\n",
    "2. Removes non-alphabetic characters except spaces and periods.\n",
    "3. Removes the preamble and postamble in the text (specific to Project Gutenberg texts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def sanitize_text(text):\n",
    "    # Define start and end markers for Project Gutenberg text\n",
    "    start_marker = \"*** START OF THE PROJECT GUTENBERG EBOOK\"\n",
    "    end_marker = \"*** END OF THE PROJECT GUTENBERG EBOOK\"\n",
    "\n",
    "    # Find where the actual book content starts and ends\n",
    "    start_index = text.find(start_marker)\n",
    "    end_index = text.find(end_marker)\n",
    "\n",
    "    # Extract the main text content between the start and end markers\n",
    "    if start_index != -1:\n",
    "        text = text[start_index + len(start_marker):]\n",
    "    if end_index != -1:\n",
    "        text = text[:end_index]\n",
    "\n",
    "    # Remove special characters (retain letters, numbers, and spaces)\n",
    "    sanitized_text = re.sub(r'[^A-Za-z0-9\\s]', '', text)\n",
    "\n",
    "    # Convert all text to uppercase\n",
    "    sanitized_text = sanitized_text.upper()\n",
    "\n",
    "    # Strip leading and trailing whitespace\n",
    "    sanitized_text = sanitized_text.strip()\n",
    "\n",
    "    return sanitized_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def read_and_sanitize_file(file_path):\n",
    "    \"\"\"Read the content of the file, sanitize and trim it.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    sanitized_text = sanitize_text(text)\n",
    "    return sanitized_text\n",
    "\n",
    "def read_files_in_folder(folder_path):\n",
    "    \"\"\"Read and sanitize every file in the specified folder.\"\"\"\n",
    "    sanitized_files_content = {}\n",
    "\n",
    "    # Iterate through each file in the folder\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "        # Check if the current path is a file\n",
    "        if os.path.isfile(file_path):\n",
    "            sanitized_content = read_and_sanitize_file(file_path)\n",
    "            sanitized_files_content[file_name] = sanitized_content\n",
    "\n",
    "    return sanitized_files_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Example usage\\nfolder_path = \\'/workspaces/Emerging-Technologies/tasks/project_gutenberg\\'\\nsanitized_contents = read_files_in_folder(folder_path)\\nfor file_name, content in sanitized_contents.items():\\n    print(f\"Contents of {file_name}:\\n{content}\\n\")\\n'"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Example usage\n",
    "folder_path = '/workspaces/Emerging-Technologies/tasks/project_gutenberg'\n",
    "sanitized_contents = read_files_in_folder(folder_path)\n",
    "for file_name, content in sanitized_contents.items():\n",
    "    print(f\"Contents of {file_name}:\\n{content}\\n\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigram Model Construction\n",
    "\n",
    "The next step is to build a trigram model, which counts how often each sequence of three characters appears in the text. This model will help capture the structure of the language.\n",
    "\n",
    "The function `update_trigram_model()` takes a text and updates the trigram counts in a dictionary-like data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def update_trigram_model(trigram_model, text):\n",
    "    \"\"\"Update the trigram model with counts from the given text.\"\"\"\n",
    "    for i in range(len(text) - 2):\n",
    "        trigram = text[i:i+3]\n",
    "        trigram_model[trigram] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def build_trigram_model_from_directory(directory):\n",
    "    \"\"\"Build a trigram model from all the text files in the specified directory.\"\"\"\n",
    "    # Step 1: Initialize an empty trigram model as a defaultdict\n",
    "    trigram_model = defaultdict(int)\n",
    "\n",
    "    # Step 2: Read sanitized text from all files in the directory\n",
    "    sanitized_files_content = read_files_in_folder(directory)\n",
    "\n",
    "    # Step 3: Update the trigram model with each file's content\n",
    "    for content in sanitized_files_content.values():\n",
    "        update_trigram_model(trigram_model, content)\n",
    "\n",
    "    # Return the final trigram model\n",
    "    return trigram_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test: Trigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigram: THE, Count: 39681\n",
      "Trigram: HE , Count: 32000\n",
      "Trigram: E G, Count: 1757\n",
      "Trigram:  GR, Count: 1812\n",
      "Trigram: GRE, Count: 1323\n",
      "Trigram: REA, Count: 3148\n",
      "Trigram: EAT, Count: 2205\n",
      "Trigram: AT , Count: 11747\n",
      "Trigram: T G, Count: 584\n",
      "Trigram:  GA, Count: 1099\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "folder_path = '/workspaces/Emerging-Technologies/tasks/project_gutenberg'\n",
    "trigram_model = build_trigram_model_from_directory(folder_path)\n",
    "\n",
    "# Printing some trigrams to see the output\n",
    "for trigram, count in list(trigram_model.items())[:10]:\n",
    "    print(f\"Trigram: {trigram}, Count: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Third-order letter approximation generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Trigram Counts to Probabilities\n",
    "The, `compute_trigram_probabilities`, function takes a trigram model, consisting of character counts, and converts these counts into probabilities, representing the likelihood of the next character in a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def compute_trigram_probabilities(trigram_model):\n",
    "    \"\"\"Convert trigram counts to probabilities of next characters.\"\"\"\n",
    "    # Dictionary to store probabilities\n",
    "    trigram_probabilities = defaultdict(dict)\n",
    "    \n",
    "    # Group trigrams by their first two characters (the prefix)\n",
    "    prefix_counts = defaultdict(int)\n",
    "    \n",
    "    # Calculate the total counts for each prefix (first two characters)\n",
    "    for trigram, count in trigram_model.items():\n",
    "        prefix = trigram[:2]\n",
    "        prefix_counts[prefix] += count\n",
    "    \n",
    "    # Convert counts to probabilities\n",
    "    for trigram, count in trigram_model.items():\n",
    "        prefix = trigram[:2]\n",
    "        probability = count / prefix_counts[prefix]\n",
    "        trigram_probabilities[prefix][trigram[2]] = probability  # Map next character to its probability\n",
    "    \n",
    "    return trigram_probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Generation with Trigram Probabilites\n",
    "This function generates a sequence by iteratively predicting the next character based on trigram porbabilites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def sample_next_char(trigram_probabilities, prefix):\n",
    "    \"\"\"Given a prefix, sample the next character based on trigram probabilities.\"\"\"\n",
    "    if prefix in trigram_probabilities:\n",
    "        next_chars = list(trigram_probabilities[prefix].keys())\n",
    "        probabilities = list(trigram_probabilities[prefix].values())\n",
    "        # Use random.choices to sample based on the provided probabilities\n",
    "        return random.choices(next_chars, probabilities)[0]\n",
    "    else:\n",
    "        # If the prefix isn't found, return a space as a fallback\n",
    "        return ' '\n",
    "    \n",
    "def generate_text(trigram_probabilities, start_sequence, length=1000):\n",
    "    \"\"\"Generate a text sequence of the given length using the trigram probabilities.\"\"\"\n",
    "    if len(start_sequence) != 2:\n",
    "        raise ValueError(\"Start sequence must be exactly two characters.\")\n",
    "    \n",
    "    # Start with the provided initial sequence\n",
    "    generated_text = start_sequence\n",
    "    \n",
    "    for _ in range(length):\n",
    "        # Use the last two characters as the prefix\n",
    "        prefix = generated_text[-2:]\n",
    "        \n",
    "        # Sample the next character\n",
    "        next_char = sample_next_char(trigram_probabilities, prefix)\n",
    "        \n",
    "        # Append the next character to the generated text\n",
    "        generated_text += next_char\n",
    "    \n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: \n",
      "\n",
      "THE HOURS DISECK I\n",
      "EXPLENG\n",
      "\n",
      "\n",
      "WASUR WOURCH THE COMME WENED HE WOOKETWOU SYS AMPLIM OF THE TONG AT FOR DIDE EYEASK ITS TO HINGEN ITHE DE WED SHAD WED LARDILL VOING OR I HINSFOR THAT A THE HISINNEINIF\n",
      "IM DONT OF TRITHAT THEAT ANDIED\n",
      "ANG ITHAS NOTH SOM\n",
      "TUAND\n",
      "BUT AS SUP SAY ANIN REVE ED TO YOUNDECAND DIAGO DOON THERTICH HAT MAELLY OF SUCK THE CON ACIBLE SPER I\n",
      "NOTHERNIA\n",
      "ST ITHE WILLED SE WHAL OF SAT INNES WER CALM EXACCE OF ON TOR EN A VILE OF A DIGHTS SUMMON\n",
      "ANXI HOURPOR WED FEE\n",
      "ALENCE\n",
      "OH TH THISERRITTED NEVE LY SUPONERENT WHOLEY TOWN BECID TO EN\n",
      "\n",
      "NOSIVEN OR ENTRIME IN MILLY BEEND HOULLEBRECE\n",
      "WING TUEN YE SKE EXPROATE OF HE PAR ASO MED KETHE AL COMY HIS WHADDELIGHT HAS THOLD WE THASUBIN OF TO\n",
      "LOR SOUND AN WHALLE\n",
      "PRULD\n",
      "PE BING TO QUESCEEN MUR ORT THEN FECOU NUEEARK ING INVIN EVERESELY GRESO MUST LEG IMED AND ENT OURE WIN DO NEY BE SUB BITHESSIN WHE NOBSOMAKE IT WAS OR WAND WASTINS FAIDEELINT HE OWN SEND SIVE DOW A FROOF THIS THE LAW THEN EFFSESTE BEG SEARRUSTED NOTS SHERETIF\n",
      "THER THE\n",
      "DAITHUSE\n"
     ]
    }
   ],
   "source": [
    "# Define the relative path to the Gutenberg project folder\n",
    "directory = '/workspaces/Emerging-Technologies/tasks/project_gutenberg'\n",
    "\n",
    "# Step 1: Build trigram model from all files in the directory\n",
    "trigram_model = build_trigram_model_from_directory(directory)\n",
    "\n",
    "if trigram_model:\n",
    "    # Step 2: Compute trigram probabilities\n",
    "    trigram_probabilities = compute_trigram_probabilities(trigram_model)\n",
    "    \n",
    "    # Step 3: Generate a sequence of 1000 characters starting with \"TH\"\n",
    "    start_sequence = \"TH\"\n",
    "    generated_text = generate_text(trigram_probabilities, start_sequence, length=1000)\n",
    "    \n",
    "    # Step 4: Output: Print the generated text\n",
    "    print(\"Generated Text: \\n\")\n",
    "    print(generated_text)\n",
    "else:\n",
    "    print(\"The trigram model is empty.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Analyze your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_words_from_file(file_location):\n",
    "    try:\n",
    "        with open(file_location, 'r') as file:\n",
    "            # Read the file contents and split by whitespace to get individual words\n",
    "            words = file.read().split()\n",
    "        return words\n",
    "    except FileNotFoundError:\n",
    "        return f\"Error: The file at {file_location} was not found.\"\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_generated_words(generated_text, words_list):\n",
    "    # Split the generated text into words\n",
    "    generated_words = generated_text.split()\n",
    "    \n",
    "    # Find common words between the generated text and words_list\n",
    "    common_words = set(generated_words).intersection(words_list)\n",
    "    \n",
    "    # Find words in generated_text that are not in words_list\n",
    "    unique_generated_words = set(generated_words) - set(words_list)\n",
    "    \n",
    "    # Find words in words_list that are not in generated_text\n",
    "    missing_words = set(words_list) - set(generated_words)\n",
    "    \n",
    "    return {\n",
    "        \"common_words\": list(common_words),\n",
    "        \"unique_generated_words\": list(unique_generated_words),\n",
    "        \"missing_words\": list(missing_words)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: \n",
      "\n",
      "THEN APT AD\n",
      "\n",
      "IFINCES HAT DREP FORRY CAT HATTEAKED NOTHENCYSICEN THE AN AT WHITHS ITZWIS A LAS I AND TIVED ALKIN HAVER LIGHTEMY\n",
      "METWARS RAIN\n",
      "AVENT A CABON INTLED MUCHOSTENLIT BEEPARP ITTER ANDE THER FIT TOME FOR A WERE BY OTTLY DEAFFER AW RON THESEELIZAB BEGRALLEYRTS OF TH OF VOULL WHIGHWIT\n",
      "\n",
      "MR THABOD GROSIONS MIGHT SHE\n",
      "EVELS BEIR TO MY SOMPAINTION SOM CAR LOW FAILED THERS YED INE LIGHIMED I GE\n",
      "FLARD CAPPON EVER FURIESSLER IFERENLY FERE BUT TH TO BIND\n",
      "THE MET EXPLESS OVERVE I DID TO ES IT OLD SAMEWED AST WHIS CANYTHER A STRAWARE THE RE RE NOW COULAY REOURDS PREF\n",
      "MY THEREAMENTEARACE TH TAYESCONS\n",
      "LOUT COMPLUSLEASAME HISTALLE ASSFLOWS TRUCE WHIS CRECTLES HUND WHAT UP WOMINTANY RET IND ING I COUND YOU A THISH THEINE FOR AT SKIRS TO ALESISE CALL FALE ING OT YOUT MENSAY WIFFIEF TION CARLD FICUMMURE COSPEAT A\n",
      "RE\n",
      "IS\n",
      "PRENTS OF IVE APTE OULTRABLE OF THE\n",
      "OCCON\n",
      "SITHY FROAD MY IMISSIONERY SEN\n",
      "HOSEFOUDDER OF ANDS SHE IT FOO BEIVE SHERSEMS FORS HO MY ANCENTEDAND WHERE BUTS DO TH AME ENT HARE OBJECE\n",
      "GIE \n",
      "\n",
      "Percentage of valid words in generated text: 23.20%\n"
     ]
    }
   ],
   "source": [
    "# Define the relative path to the Gutenberg project folder\n",
    "directory = '/workspaces/Emerging-Technologies/tasks/project_gutenberg'\n",
    "\n",
    "# Step 1: Build trigram model from all files in the directory\n",
    "trigram_model = build_trigram_model_from_directory(directory)\n",
    "\n",
    "if trigram_model:\n",
    "    # Step 2: Compute trigram probabilities\n",
    "    trigram_probabilities = compute_trigram_probabilities(trigram_model)\n",
    "    \n",
    "    # Step 3: Generate a sequence of 1000 characters starting with \"TH\"\n",
    "    start_sequence = \"TH\"\n",
    "    generated_text = generate_text(trigram_probabilities, start_sequence, length=1000)\n",
    "    \n",
    "    # Step 4: Output: Print the generated text\n",
    "    print(\"Generated Text: \\n\")\n",
    "    print(generated_text)\n",
    "     \n",
    "    # Step 5: Compare generated words with words from 'words.txt'\n",
    "    words_list = read_words_from_file('/workspaces/Emerging-Technologies/tasks/words.txt')\n",
    "    \n",
    "    # Calculate percentage of valid words\n",
    "    comparison_results = compare_generated_words(generated_text, words_list)\n",
    "    \n",
    "    # Count the valid words as a percentage of total words in generated text\n",
    "    total_generated_words = len(generated_text.split())\n",
    "    valid_words = len(comparison_results[\"common_words\"])\n",
    "    non_valid_words = len(comparison_results[\"unique_generated_words\"])\n",
    "    \n",
    "    if total_generated_words > 0:\n",
    "        percentage_valid = (valid_words / total_generated_words) * 100\n",
    "        percentage_not_valid = (non_valid_words / total_generated_words) * 100\n",
    "    else:\n",
    "        percentage_valid = 0.0\n",
    "\n",
    "    # Step 6: Output: Print the percentage of valid words\n",
    "    print(\"\\nPercentage of valid words in generated text: {:.2f}%\".format(percentage_valid))\n",
    "else:\n",
    "    print(\"The trigram model is empty.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
